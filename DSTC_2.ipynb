{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/formula.jws.json\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/cask.jws.json\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[33mWarning:\u001b[0m wget 1.24.5 is already installed and up-to-date.\n",
      "To reinstall 1.24.5, run:\n",
      "  brew reinstall wget\n",
      "--2024-06-05 23:46:28--  https://github.com/matthen/dstc/releases/download/v1/dstc2_traindev.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/295903958/9cf74180-f80b-11ea-997a-a3b76087e723?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T214628Z&X-Amz-Expires=300&X-Amz-Signature=fab4b2ca4ddb366daefc16373f06c9f215d2c2f86b294933629d35dd3d71a08f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=295903958&response-content-disposition=attachment%3B%20filename%3Ddstc2_traindev.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-06-05 23:46:28--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/295903958/9cf74180-f80b-11ea-997a-a3b76087e723?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T214628Z&X-Amz-Expires=300&X-Amz-Signature=fab4b2ca4ddb366daefc16373f06c9f215d2c2f86b294933629d35dd3d71a08f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=295903958&response-content-disposition=attachment%3B%20filename%3Ddstc2_traindev.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31917509 (30M) [application/octet-stream]\n",
      "Saving to: ‘dstc2_traindev.tar.gz’\n",
      "\n",
      "dstc2_traindev.tar. 100%[===================>]  30.44M  42.1MB/s    in 0.7s    \n",
      "\n",
      "2024-06-05 23:46:29 (42.1 MB/s) - ‘dstc2_traindev.tar.gz’ saved [31917509/31917509]\n",
      "\n",
      "--2024-06-05 23:46:29--  https://github.com/matthen/dstc/releases/download/v1/dstc2_test.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/295903958/95d03380-f80b-11ea-88b4-98ce4f218d5e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T214630Z&X-Amz-Expires=300&X-Amz-Signature=85354c2259c28d17c3ea03474d2ddc42dcf11e6f37d7aecae3e4afb4d19c39ec&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=295903958&response-content-disposition=attachment%3B%20filename%3Ddstc2_test.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-06-05 23:46:30--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/295903958/95d03380-f80b-11ea-88b4-98ce4f218d5e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T214630Z&X-Amz-Expires=300&X-Amz-Signature=85354c2259c28d17c3ea03474d2ddc42dcf11e6f37d7aecae3e4afb4d19c39ec&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=295903958&response-content-disposition=attachment%3B%20filename%3Ddstc2_test.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20180642 (19M) [application/octet-stream]\n",
      "Saving to: ‘dstc2_test.tar.gz’\n",
      "\n",
      "dstc2_test.tar.gz   100%[===================>]  19.25M  41.2MB/s    in 0.5s    \n",
      "\n",
      "2024-06-05 23:46:30 (41.2 MB/s) - ‘dstc2_test.tar.gz’ saved [20180642/20180642]\n",
      "\n",
      "--2024-06-05 23:46:30--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2024-06-05 23:46:31--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  3.53MB/s    in 5m 34s  \n",
      "\n",
      "2024-06-05 23:52:06 (2.46 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downloading the dataset and the Glove word embeddings\n",
    "! brew install wget\n",
    "! wget https://github.com/matthen/dstc/releases/download/v1/dstc2_traindev.tar.gz\n",
    "! wget https://github.com/matthen/dstc/releases/download/v1/dstc2_test.tar.gz\n",
    "! wget https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the train and test data from the tar.gz files\n",
    "import tarfile\n",
    "\n",
    "train_tar_path = 'dstc2_traindev.tar.gz'\n",
    "test_tar_path = 'dstc2_test.tar.gz'\n",
    "\n",
    "# Extracting the tar.gz files to the dstc2_traindev and dstc2_test directories\n",
    "with tarfile.open(train_tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path='dstc2_traindev')\n",
    "\n",
    "with tarfile.open(test_tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path='dstc2_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON files\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_highest_score_asr(asr_hyps):\n",
    "    return max(asr_hyps, key=lambda hyp: hyp['score'])\n",
    "\n",
    "def extract_data(label, log):\n",
    "    extracted_data = []\n",
    "    for turn in log['turns']:\n",
    "        highest_score_asr = get_highest_score_asr(turn['input']['live']['asr-hyps'])\n",
    "        dialog_act_text = highest_score_asr['asr-hyp']\n",
    "        turn_index = turn['turn-index']\n",
    "        corresponding_label_turn = next(t for t in label['turns'] if t['turn-index'] == turn_index)\n",
    "        dialog_act_label = corresponding_label_turn['semantics']['cam']\n",
    "        \n",
    "        extracted_data.append({\n",
    "            \"dialog_act_text\": dialog_act_text,\n",
    "            \"dialog_act_label\": dialog_act_label\n",
    "        })\n",
    "    return extracted_data\n",
    "\n",
    "def process_all_files(base_dir):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if 'label.json' in files and 'log.json' in files:\n",
    "            label_path = os.path.join(root, 'label.json')\n",
    "            log_path = os.path.join(root, 'log.json')\n",
    "            label = load_json(label_path)\n",
    "            log = load_json(log_path)\n",
    "            session_data = extract_data(label, log)\n",
    "            data.extend(session_data)\n",
    "    return data\n",
    "\n",
    "base_train_dir = 'dstc2_traindev/data'\n",
    "base_test_dir = 'dstc2_test/data'\n",
    "\n",
    "train_data = process_all_files(base_train_dir)\n",
    "test_data = process_all_files(base_test_dir)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# save the train and test dfs to a csv file\n",
    "train_df.to_csv('train_data.csv', index=True)\n",
    "test_df.to_csv('test_data.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the glove embeddings file\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_file_path = 'glove.6B/glove.6B.100d.txt'\n",
    "word_embeddings = load_glove_embeddings(glove_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, text, labels, embeddings):\n",
    "#         self.text = text\n",
    "#         self.labels = labels\n",
    "#         self.embeddings = embeddings\n",
    "#         self.dim = len(next(iter(embeddings.values())))  # Dimension of embeddings\n",
    "#         self.label_to_int = {label: i for i, label in enumerate(set(labels))}  # Create mapping from labels to integers\n",
    "                \n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sentence = self.text[idx]\n",
    "#         embedding_vectors = [self.embeddings[word] for word in sentence.split() if word in self.embeddings]\n",
    "#         if embedding_vectors:\n",
    "#             sentence_embedding = np.mean(embedding_vectors, axis=0)\n",
    "#         else:\n",
    "#             # If none of the words in the sentence are in the embeddings, return zeros\n",
    "#             sentence_embedding = np.zeros(self.dim)\n",
    "#         label = self.label_to_int[self.labels[idx]] / len(self.label_to_int)  # Normalize label to be between 0 and 1\n",
    "#         print(f\"Label: {label}\")\n",
    "#         return torch.tensor(sentence_embedding, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, labels, embeddings, dim):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.embeddings = embeddings\n",
    "        self.dim = dim\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.text[idx]\n",
    "        embedding_vectors = [self.embeddings[word.lower()] for word in sentence.split() if word.lower() in self.embeddings]\n",
    "        if embedding_vectors:\n",
    "            sentence_embedding = np.mean(embedding_vectors, axis=0)\n",
    "        else:\n",
    "            sentence_embedding = np.zeros(self.dim)\n",
    "        label = 1 if self.labels[idx] == \"inform\" else 0  # Convert labels to binary\n",
    "        return torch.tensor(sentence_embedding, dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "dim = len(next(iter(word_embeddings.values())))\n",
    "train_dataset = TextDataset(train_df['dialog_act_text'], train_df['dialog_act_label'], word_embeddings, dim)\n",
    "test_dataset = TextDataset(test_df['dialog_act_text'], test_df['dialog_act_label'], word_embeddings, dim)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 15611\n",
      "Number of test examples: 9890\n",
      "Embedding dimension: 100\n",
      "Number of train dataloader batches: 488\n",
      "Number of test dataloader batches: 310\n",
      "(tensor([-0.1284,  0.2365,  0.0348, -0.2114, -0.0207,  0.4156,  0.0400,  0.0508,\n",
      "         0.2099,  0.1153, -0.2247, -0.0582,  0.2855,  0.1336,  0.1801,  0.1997,\n",
      "         0.1618, -0.1618, -0.6136,  0.1638,  0.2289,  0.2042,  0.0741, -0.2789,\n",
      "         0.1049,  0.3242, -0.2329, -0.4341, -0.3022,  0.0707, -0.2982,  0.4931,\n",
      "         0.0655,  0.0609,  0.1835,  0.8005, -0.0021,  0.3045, -0.3090, -0.5727,\n",
      "         0.0290, -0.0228, -0.0842, -0.6062,  0.4922,  0.1210, -0.7145, -0.1770,\n",
      "         0.1424, -0.2483, -0.1107, -0.1112, -0.3060,  0.1717, -0.4653, -1.3783,\n",
      "        -0.0728, -0.0912,  1.5972,  0.0706, -0.1195,  0.1731,  0.0717, -0.2333,\n",
      "         0.5830, -0.1154,  0.2160, -0.4106,  0.1289, -0.4419, -0.3996, -0.1192,\n",
      "         0.0616,  0.1092, -0.3175,  0.0700, -0.3186, -0.2148, -0.6008, -0.0552,\n",
      "         0.2669, -0.0161, -0.0817,  0.3936, -0.7411, -0.1377,  0.0906,  0.2215,\n",
      "        -0.6095, -0.2556,  0.3145, -0.0823,  0.1874, -0.3687, -0.6120, -0.2286,\n",
      "         0.1992, -0.3178,  0.6623,  0.1598]), tensor(0.))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "print(f\"Number of test examples: {len(test_dataset)}\")\n",
    "print(f\"Embedding dimension: {dim}\")\n",
    "print(\"Number of train dataloader batches:\", len(train_dataloader))\n",
    "print(\"Number of test dataloader batches:\", len(test_dataloader))\n",
    "print(train_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y.float().unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y.float().unsqueeze(1)).item()\n",
    "            correct += ((pred > 0.5) == y.unsqueeze(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.688425  [    0/15611]\n",
      "loss: 0.009006  [ 3200/15611]\n",
      "loss: 0.000002  [ 6400/15611]\n",
      "loss: 0.004066  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000128 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/15611]\n",
      "loss: 0.000002  [ 3200/15611]\n",
      "loss: 0.000141  [ 6400/15611]\n",
      "loss: 0.000105  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000020 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000117  [    0/15611]\n",
      "loss: 0.000000  [ 3200/15611]\n",
      "loss: 0.000000  [ 6400/15611]\n",
      "loss: 0.000030  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000007 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.000064  [    0/15611]\n",
      "loss: 0.000000  [ 3200/15611]\n",
      "loss: 0.000000  [ 6400/15611]\n",
      "loss: 0.000000  [ 9600/15611]\n",
      "loss: 0.000012  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000004 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/15611]\n",
      "loss: 0.000000  [ 3200/15611]\n",
      "loss: 0.000000  [ 6400/15611]\n",
      "loss: 0.000007  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000002 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/15611]\n",
      "loss: 0.000000  [ 3200/15611]\n",
      "loss: 0.000005  [ 6400/15611]\n",
      "loss: 0.000000  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000001 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/15611]\n",
      "loss: 0.000000  [ 3200/15611]\n",
      "loss: 0.000000  [ 6400/15611]\n",
      "loss: 0.000003  [ 9600/15611]\n",
      "loss: 0.000003  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000001 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/15611]\n",
      "loss: 0.000000  [ 3200/15611]\n",
      "loss: 0.000000  [ 6400/15611]\n",
      "loss: 0.000000  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000001 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/15611]\n",
      "loss: 0.000000  [ 3200/15611]\n",
      "loss: 0.000000  [ 6400/15611]\n",
      "loss: 0.000000  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000000 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/15611]\n",
      "loss: 0.000001  [ 3200/15611]\n",
      "loss: 0.000001  [ 6400/15611]\n",
      "loss: 0.000000  [ 9600/15611]\n",
      "loss: 0.000000  [12800/15611]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000000 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "input_dim = 100\n",
    "model = NeuralNet(input_dim)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to test_data_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        pred = model(X)\n",
    "        predictions.extend(pred)\n",
    "\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "test_df['predictions'] = predictions\n",
    "test_df.to_csv('test_data_with_predictions.csv', index=True)\n",
    "print(\"Predictions saved to test_data_with_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the downloaded files so that we can upload to the github repo\n",
    "! rm -rf dstc2_traindev\n",
    "! rm -rf dstc2_test\n",
    "! rm dstc2_traindev.tar.gz\n",
    "! rm dstc2_test.tar.gz\n",
    "! rm glove.6B.zip\n",
    "! rm -rf glove.6B\n",
    "! rm train_data.csv\n",
    "! rm test_data.csv\n",
    "! rm test_data_with_predictions.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
